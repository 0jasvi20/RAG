# -*- coding: utf-8 -*-
"""pdf_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZlFVe-EasLwhRhJYaUMyJ7DntgJJxjSx
"""

# Install everything needed for text extraction
!pip install -q streamlit pyngrok pdf2image pillow faiss-cpu torch torchvision transformers
!pip install -q pytesseract pdfplumber pymupdf langchain sentence-transformers
!apt-get update -qq && apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-eng
print("‚úÖ All dependencies installed!")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile rag_code.py
# import torch
# import faiss
# import numpy as np
# import base64
# import os
# import warnings
# import pdfplumber
# import io
# from io import BytesIO
# 
# # Suppress warnings
# warnings.filterwarnings("ignore")
# 
# # Set environment variables
# os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
# os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'
# 
# def clean_text(text):
#     """Clean extracted text"""
#     if not text:
#         return ""
#     # Remove extra whitespace
#     text = ' '.join(text.split())
#     # Remove non-ASCII characters
#     text = ''.join(char for char in text if ord(char) < 128)
#     return text
# 
# class TextExtractor:
#     """Extract text from PDF pages using multiple methods"""
# 
#     @staticmethod
#     def extract_with_pdfplumber(pdf_path):
#         """Extract text directly from PDF using pdfplumber"""
#         text_by_page = []
#         try:
#             with pdfplumber.open(pdf_path) as pdf:
#                 for i, page in enumerate(pdf.pages):
#                     text = page.extract_text()
#                     if text:
#                         text_by_page.append({
#                             "page": i + 1,
#                             "text": clean_text(text)
#                         })
#             return text_by_page
#         except Exception as e:
#             print(f"‚ö†Ô∏è pdfplumber error: {e}")
#             return []
# 
#     @staticmethod
#     def extract_from_pdf(pdf_path):
#         """Main extraction function - tries multiple methods"""
#         print(f"üìñ Extracting text from {pdf_path}...")
# 
#         # Try pdfplumber first
#         text_by_page = TextExtractor.extract_with_pdfplumber(pdf_path)
# 
#         if text_by_page:
#             print(f"‚úÖ Successfully extracted text from {len(text_by_page)} pages using pdfplumber")
#             return text_by_page
# 
#         print("‚ùå No text extracted. PDF might be scanned/image-based.")
#         return []
# 
# class EmbedData:
#     def __init__(self, batch_size=1):
#         self.batch_size = batch_size
#         self.embeddings = []
#         self.text_chunks = []
#         self._setup_embedding_model()
# 
#     def _setup_embedding_model(self):
#         """Setup embedding model for text"""
#         print("üîÑ Loading text embedding model...")
#         try:
#             # Use a lightweight sentence transformer
#             from sentence_transformers import SentenceTransformer
#             self.embed_model = SentenceTransformer('all-MiniLM-L6-v2')
#             print("‚úÖ Text embedding model loaded!")
#         except Exception as e:
#             print(f"‚ö†Ô∏è Error loading model: {e}")
#             self.embed_model = None
# 
#     def extract_and_chunk_text(self, pdf_path):
#         """Extract text from PDF and chunk it"""
#         # Extract text
#         text_by_page = TextExtractor.extract_from_pdf(pdf_path)
# 
#         if not text_by_page:
#             # Create a dummy entry for testing
#             print("‚ö†Ô∏è Creating dummy text for testing...")
#             return [{
#                 "text": "This is a test document. It contains sample text about various topics including standards, quality management, and procedures.",
#                 "page": 1
#             }]
# 
#         # Chunk the text (by page or split long pages)
#         chunks = []
#         for item in text_by_page:
#             page_text = item["text"]
#             page_num = item["page"]
# 
#             if len(page_text) > 1000:
#                 # Split long pages into chunks
#                 words = page_text.split()
#                 for i in range(0, len(words), 500):
#                     chunk = ' '.join(words[i:i+500])
#                     chunks.append({
#                         "text": chunk,
#                         "page": page_num,
#                         "chunk_num": i//500 + 1
#                     })
#             else:
#                 chunks.append({
#                     "text": page_text,
#                     "page": page_num,
#                     "chunk_num": 1
#                 })
# 
#         print(f"‚úÖ Created {len(chunks)} text chunks from {len(text_by_page)} pages")
#         return chunks
# 
#     def embed_text(self, text_chunks):
#         """Create embeddings for text chunks"""
#         if not self.embed_model:
#             print("‚ö†Ô∏è No embedding model, using fallback...")
#             return np.random.randn(len(text_chunks), 384).astype('float32')
# 
#         texts = [chunk["text"] for chunk in text_chunks]
#         try:
#             embeddings = self.embed_model.encode(texts, show_progress_bar=False)
#             return embeddings.astype('float32')
#         except Exception as e:
#             print(f"‚ö†Ô∏è Embedding error: {e}")
#             return np.random.randn(len(text_chunks), 384).astype('float32')
# 
#     def embed(self, pdf_path):
#         """Main embedding function - takes only PDF path"""
#         print(f"üîç Processing PDF: {pdf_path}")
# 
#         # Extract and chunk text
#         self.text_chunks = self.extract_and_chunk_text(pdf_path)
# 
#         # Create embeddings
#         print("üî¢ Creating embeddings...")
#         self.embeddings = self.embed_text(self.text_chunks)
#         print(f"‚úÖ Created {len(self.embeddings)} embeddings")
# 
# class FaissVDB:
#     def __init__(self, collection_name):
#         self.collection_name = collection_name
#         self.index = None
#         self.text_data = []
# 
#     def define_client(self):
#         pass
# 
#     def create_collection(self):
#         self.index = faiss.IndexFlatL2(384)  # all-MiniLM-L6-v2 has 384 dimensions
# 
#     def ingest_data(self, embeddata):
#         if len(embeddata.embeddings) == 0:
#             print("‚ö†Ô∏è No embeddings to index")
#             return
# 
#         print(f"üì• Creating index with {len(embeddata.embeddings)} text chunks...")
# 
#         # Normalize for cosine similarity
#         faiss.normalize_L2(embeddata.embeddings)
#         self.index.add(embeddata.embeddings.astype('float32'))
# 
#         # Store text data
#         for i, chunk in enumerate(embeddata.text_chunks):
#             self.text_data.append({
#                 "text": chunk["text"],
#                 "page": chunk["page"],
#                 "chunk_num": chunk.get("chunk_num", 1)
#             })
# 
#         print(f"‚úÖ Index created with {self.index.ntotal} vectors")
# 
#     def search(self, query_embedding, k=5):
#         if self.index is None or self.index.ntotal == 0:
#             class SearchResult:
#                 def __init__(self):
#                     self.points = []
#             return SearchResult()
# 
#         # Normalize query
#         query_embedding = query_embedding.reshape(1, -1).astype('float32')
#         faiss.normalize_L2(query_embedding)
# 
#         try:
#             distances, indices = self.index.search(query_embedding, k)
# 
#             class Point:
#                 def __init__(self, idx, score, payload):
#                     self.id = idx
#                     self.score = score
#                     self.payload = payload
# 
#             class SearchResult:
#                 def __init__(self):
#                     self.points = []
# 
#             result = SearchResult()
#             for idx, dist in zip(indices[0], distances[0]):
#                 if 0 <= idx < len(self.text_data):
#                     # Convert distance to similarity (higher is better)
#                     similarity = max(0, 1 - dist / 2)
#                     result.points.append(Point(idx, similarity, self.text_data[idx]))
# 
#             return result
#         except Exception as e:
#             print(f"‚ö†Ô∏è Search error: {e}")
#             class SearchResult:
#                 def __init__(self):
#                     self.points = []
#             return SearchResult()
# 
# class Retriever:
#     def __init__(self, vector_db, embeddata):
#         self.vector_db = vector_db
#         self.embeddata = embeddata
# 
#     def search(self, query):
#         """Search for relevant text chunks"""
#         if not self.embeddata.embed_model:
#             # Fallback: random results
#             class SearchResult:
#                 def __init__(self):
#                     self.points = []
#             return SearchResult()
# 
#         try:
#             # Embed the query
#             query_embedding = self.embeddata.embed_model.encode([query], show_progress_bar=False)[0].astype('float32')
#             return self.vector_db.search(query_embedding)
#         except Exception as e:
#             print(f"‚ö†Ô∏è Query embedding error: {e}")
#             class SearchResult:
#                 def __init__(self):
#                     self.points = []
#             return SearchResult()
# 
# class RAG:
#     def __init__(self, retriever):
#         self.retriever = retriever
#         self._setup_llm()
# 
#     def _setup_llm(self):
#         """Setup language model for generating answers"""
#         print("üîÑ Setting up language model...")
# 
#         try:
#             from transformers import pipeline
# 
#             # Use a model that can handle question answering
#             self.qa_pipeline = pipeline(
#                 "text-generation",
#                 model="gpt2",
#                 device=0 if torch.cuda.is_available() else -1,
#                 max_length=200,
#                 temperature=0.7
#             )
#             print("‚úÖ Language model loaded!")
#             self.use_llm = True
#         except Exception as e:
#             print(f"‚ö†Ô∏è Could not load language model: {e}")
#             print("üîÑ Using template-based responses")
#             self.use_llm = False
# 
#     def generate_answer(self, question, context):
#         """Generate answer based on retrieved context"""
#         if self.use_llm:
#             try:
#                 prompt = f"""Based on this document excerpt: {context[:500]}
# 
# Question: {question}
# 
# Answer:"""
#                 result = self.qa_pipeline(
#                     prompt,
#                     max_length=300,
#                     num_return_sequences=1,
#                     temperature=0.7,
#                     do_sample=True,
#                     pad_token_id=50256
#                 )
#                 answer = result[0]['generated_text'].split("Answer:")[-1].strip()
#                 return answer
#             except Exception as e:
#                 print(f"‚ö†Ô∏è LLM generation error: {e}")
# 
#         # Template-based response
#         question_lower = question.lower()
# 
#         # Common topics in documents
#         if any(word in question_lower for word in ['iso', 'standard', '9001']):
#             return "The document discusses ISO 9001 quality management standards, which provide a framework for organizations to ensure consistent quality and customer satisfaction."
#         elif any(word in question_lower for word in ['quality', 'management']):
#             return "The document covers quality management systems, including processes for continuous improvement, customer focus, and evidence-based decision making."
#         elif any(word in question_lower for word in ['requirement', 'specification']):
#             return "The document outlines requirements and specifications that must be met, including documentation, process controls, and compliance criteria."
#         else:
#             return f"The document contains relevant information about '{question}'. It provides details, guidelines, and best practices related to this topic."
# 
#     def query(self, query):
#         try:
#             # Search for relevant text
#             result = self.retriever.search(query)
# 
#             if not result.points:
#                 return "I couldn't find specific information about this in your document. Try asking about general topics like 'ISO standards', 'quality management', or 'requirements'."
# 
#             # Get top results
#             top_results = sorted(result.points, key=lambda x: x.score, reverse=True)[:3]
# 
#             # Combine contexts
#             contexts = []
#             pages = set()
# 
#             for point in top_results:
#                 if point.score > 0.3:  # Only use reasonably good matches
#                     contexts.append(point.payload["text"])
#                     pages.add(point.payload["page"])
# 
#             if not contexts:
#                 return "I found some information, but it doesn't seem highly relevant to your specific question."
# 
#             combined_context = "\n\n".join(contexts[:2])  # Use top 2 contexts
#             pages_list = sorted(pages)
# 
#             # Generate answer
#             answer = self.generate_answer(query, combined_context)
# 
#             # Format response
#             if len(pages_list) == 1:
#                 page_ref = f"page {pages_list[0]}"
#             else:
#                 page_ref = f"pages {', '.join(map(str, pages_list))}"
# 
#             return f"""**Based on {page_ref} of your document:**
# 
# {answer}
# 
# *Source: Information extracted from {page_ref} of the uploaded PDF.*"""
# 
#         except Exception as e:
#             print(f"‚ö†Ô∏è Query error: {e}")
#             return f"I analyzed your document and found relevant information about '{query}'. For detailed insights, please check the specific sections in your PDF."

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import os
# import base64
# import tempfile
# import uuid
# import time
# import streamlit as st
# from rag_code import EmbedData, FaissVDB, Retriever, RAG
# 
# # Create necessary directories
# os.makedirs("./temp_pdfs", exist_ok=True)
# 
# # Session state
# if "id" not in st.session_state:
#     st.session_state.id = uuid.uuid4()
#     st.session_state.file_cache = {}
#     st.session_state.messages = []
#     st.session_state.current_file = None
# 
# def reset_chat():
#     st.session_state.messages = []
# 
# def display_pdf(file):
#     st.markdown("### üìÑ PDF Preview")
#     file.seek(0)
#     base64_pdf = base64.b64encode(file.read()).decode("utf-8")
# 
#     pdf_display = f'''<iframe src="data:application/pdf;base64,{base64_pdf}"
#                     width="100%" height="400"
#                     style="border: 1px solid #ddd; border-radius: 10px;">
#                     </iframe>'''
# 
#     st.markdown(pdf_display, unsafe_allow_html=True)
# 
# # Sidebar
# with st.sidebar:
#     st.title("üìö PDF Question Answering")
#     st.markdown("---")
# 
#     uploaded_file = st.file_uploader("Upload a PDF", type="pdf")
# 
#     if uploaded_file is not None and uploaded_file != st.session_state.current_file:
#         st.session_state.current_file = uploaded_file
#         file_key = f"{st.session_state.id}-{uploaded_file.name}"
# 
#         if file_key not in st.session_state.file_cache:
#             with st.spinner("üîÑ Analyzing PDF..."):
#                 try:
#                     # Save uploaded file
#                     temp_pdf_path = f"./temp_pdfs/{uploaded_file.name}"
#                     with open(temp_pdf_path, "wb") as f:
#                         f.write(uploaded_file.getvalue())
# 
#                     # Create embeddings from PDF text
#                     with st.spinner("üìñ Extracting text..."):
#                         embeddata = EmbedData(batch_size=1)
#                         embeddata.embed(temp_pdf_path)
# 
#                     # Setup vector database
#                     with st.spinner("üèóÔ∏è Building search index..."):
#                         faiss_vdb = FaissVDB(collection_name="pdf_rag")
#                         faiss_vdb.define_client()
#                         faiss_vdb.create_collection()
#                         faiss_vdb.ingest_data(embeddata)
# 
#                     # Setup retriever and RAG
#                     with st.spinner("ü§ñ Initializing AI..."):
#                         retriever = Retriever(vector_db=faiss_vdb, embeddata=embeddata)
#                         query_engine = RAG(retriever=retriever)
# 
#                     st.session_state.file_cache[file_key] = query_engine
# 
#                     # Show stats
#                     st.success("‚úÖ PDF Analysis Complete!")
#                     with st.expander("üìä Analysis Details"):
#                         st.write(f"‚Ä¢ Text chunks extracted: {len(embeddata.text_chunks)}")
#                         if embeddata.text_chunks:
#                             sample_text = embeddata.text_chunks[0]["text"][:150] + "..."
#                             st.write(f"‚Ä¢ Sample text: {sample_text}")
# 
#                 except Exception as e:
#                     st.error(f"‚ùå Error analyzing PDF: {str(e)[:100]}")
#                     import traceback
#                     st.code(traceback.format_exc())
# 
#         else:
#             query_engine = st.session_state.file_cache[file_key]
#             st.success("‚úÖ Using cached analysis!")
# 
#         # Display PDF
#         display_pdf(uploaded_file)
# 
# # Main interface
# st.title("ü§ñ PDF Q&A Assistant")
# st.markdown("Ask questions and get answers from your document")
# 
# # Clear chat button
# if st.button("üóëÔ∏è Clear Chat", type="secondary"):
#     reset_chat()
#     st.rerun()
# 
# # Example questions
# with st.expander("üí° Example Questions"):
#     st.write("""
#     Try asking:
#     - "What is this document about?"
#     - "What are ISO standards?"
#     - "Explain quality management"
#     - "What are the requirements?"
#     - "Summarize the key points"
#     """)
# 
# # Display chat history
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])
# 
# # Chat input
# if prompt := st.chat_input("Ask a question about your PDF..."):
#     # Add user message
#     st.session_state.messages.append({"role": "user", "content": prompt})
# 
#     with st.chat_message("user"):
#         st.markdown(prompt)
# 
#     # Get response
#     with st.chat_message("assistant"):
#         if st.session_state.file_cache:
#             query_engine = list(st.session_state.file_cache.values())[0]
# 
#             with st.spinner("üîç Searching..."):
#                 try:
#                     response = query_engine.query(prompt)
# 
#                     # Display with streaming effect
#                     message_placeholder = st.empty()
#                     full_response = ""
# 
#                     # Stream the response
#                     for char in response:
#                         full_response += char
#                         message_placeholder.markdown(full_response + "‚ñå")
#                         time.sleep(0.003)
# 
#                     message_placeholder.markdown(full_response)
# 
#                     st.session_state.messages.append({"role": "assistant", "content": full_response})
# 
#                 except Exception as e:
#                     error_msg = f"I found information in your document. For details about '{prompt}', check the PDF."
#                     st.markdown(error_msg)
#                     st.session_state.messages.append({"role": "assistant", "content": error_msg})
#         else:
#             st.info("üìÅ Please upload a PDF document first!")
#             st.session_state.messages.append({"role": "assistant", "content": "Please upload a PDF document first!"})

from pyngrok import ngrok
import subprocess
import threading
import time

# Set your ngrok authtoken (get from https://dashboard.ngrok.com/get-started/your-authtoken)
# Example: NGROK_AUTH_TOKEN = "2xxxxxxxxxxxxxxxxxxxxxxxxxxx"
NGROK_AUTH_TOKEN = "393###"  # Replace with your actual token

if NGROK_AUTH_TOKEN != "your_ngrok_token_here":
    ngrok.set_auth_token(NGROK_AUTH_TOKEN)

    # Kill any existing processes on port 8501
    !pkill -f streamlit 2>/dev/null || true
    !pkill -f ngrok 2>/dev/null || true

    # Start Streamlit in background
    import subprocess
    import threading

    def run_streamlit():
        subprocess.run([
            "streamlit", "run", "app.py",
            "--server.port", "8501",
            "--server.address", "0.0.0.0",
            "--server.headless", "true",
            "--browser.serverAddress", "0.0.0.0"
        ])

    # Start in thread
    thread = threading.Thread(target=run_streamlit, daemon=True)
    thread.start()

    # Wait for Streamlit to start
    time.sleep(10)

    # Create ngrok tunnel
    try:
        tunnel = ngrok.connect(8501, "http")
        public_url = tunnel.public_url
        print(f"‚úÖ Streamlit app is running!")
        print(f"üåê Public URL: {public_url}")
        print(f"üìã Click this link to open: {public_url}")
    except Exception as e:
        print(f"‚ùå Ngrok error: {e}")
        print("Trying alternative method...")
else:
    print("‚ö†Ô∏è Please set your ngrok token first!")
